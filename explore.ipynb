{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caba503",
   "metadata": {},
   "source": [
    "## Set up libraries and connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "223644a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import zipfile\n",
    "import tempfile\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from io import BytesIO\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import ( StructType, StructField, LongType, StringType, TimestampType )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e3547fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "TENANT_ID = os.getenv('TENANT_ID')\n",
    "CLIENT_ID = os.getenv('APP_ID')\n",
    "CLIENT_SECRET = os.getenv('SECRET_VALUE')\n",
    "\n",
    "SITE_ID = os.getenv('SITE_ID')\n",
    "DRIVE_ID = os.getenv('DRIVE_ID')\n",
    "DEV_WS_ID = os.getenv('DEV_WS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a0710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Token_url = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/Token\"\n",
    "Token_data = {\n",
    "    \"client_id\": CLIENT_ID,\n",
    "    \"client_secret\": CLIENT_SECRET,\n",
    "    \"grant_type\": \"client_credentials\",\n",
    "    \"scope\": \"https://graph.microsoft.com/.default\",\n",
    "}\n",
    "Token_resp = requests.post(Token_url, data=Token_data)\n",
    "if Token_resp.status_code != 200:\n",
    "    raise RuntimeError(f\"Gagal mendapatkan Token: {Token_resp.status_code} {Token_resp.text}\")\n",
    "\n",
    "Token = Token_resp.json()[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c80dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Site ID from: https://graph.microsoft.com/v1.0/sites/siloamhospitals.sharepoint.com:/sites/DataEngineering\n",
      "✅ Successfully retrieved Site ID!\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "SHAREPOINT_DOMAIN = \"siloamhospitals.sharepoint.com\"\n",
    "SITE_NAME = \"DataEngineering\"\n",
    "\n",
    "# Headers\n",
    "headers = {\"Authorization\": f\"Bearer {Token}\"}\n",
    "\n",
    "# Get Site ID\n",
    "site_id_url = f\"https://graph.microsoft.com/v1.0/sites/{SHAREPOINT_DOMAIN}:/sites/{SITE_NAME}\"\n",
    "print(f\"Fetching Site ID from: {site_id_url}\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(site_id_url, headers=headers, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    site_data = response.json()\n",
    "    site_id = site_data['id']\n",
    "    \n",
    "    print(\"✅ Successfully retrieved Site ID!\")\n",
    "    \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"❌ Timeout - Request took too long\")\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"❌ HTTP Error: {e}\")\n",
    "    if response:\n",
    "        print(f\"Response: {response.text}\")\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(f\"❌ Connection Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected Error: {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0a20b",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edffd2",
   "metadata": {},
   "source": [
    "### All function for reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backup_file_in_sharepoint(file_id, file_name, parent_folder_id, headers, SITE_ID, DRIVE_ID, backup_folder_path):\n",
    "    \"\"\"\n",
    "    Create a backup copy of a file in SharePoint before reading it.\n",
    "    Backup file will be named: original_name_YYYYMMDD_HHMMSS_Backup.ext\n",
    "    \n",
    "    Returns:\n",
    "    - dict with backup info if successful\n",
    "    - Raises Exception if backup fails (stops the process)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the backup folder ID (where backup will be saved)\n",
    "        backup_parent_id = get_parent_folder_id(backup_folder_path, headers, SITE_ID, DRIVE_ID)\n",
    "        \n",
    "        if not backup_parent_id:\n",
    "            raise Exception(f\"Backup folder not found: {backup_folder_path}\")\n",
    "        \n",
    "        # Generate backup name with timestamp (include time to avoid duplicates)\n",
    "        timestamp = (datetime.now(timezone.utc) + timedelta(hours=7)).strftime(\"%Y%m%d_%H%M%S\")\n",
    "        name_parts = file_name.rsplit('.', 1)\n",
    "        if len(name_parts) == 2:\n",
    "            backup_name = f\"{name_parts[0]}_{timestamp}_Backup.{name_parts[1]}\"\n",
    "        else:\n",
    "            backup_name = f\"{file_name}_{timestamp}_Backup\"\n",
    "        \n",
    "        print(f\"  → Creating backup: {backup_name}\")\n",
    "        print(f\"  → Backup location: {backup_folder_path}\")\n",
    "        \n",
    "        copy_url = f\"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/items/{file_id}/copy\"\n",
    "        copy_body = {\n",
    "            \"parentReference\": {\"id\": backup_parent_id},  # Changed from parent_folder_id\n",
    "            \"name\": backup_name\n",
    "        }\n",
    "        \n",
    "        response = requests.post(copy_url, headers=headers, json=copy_body)\n",
    "        \n",
    "        if response.status_code != 202:\n",
    "            # Backup failed - STOP PROCESS\n",
    "            error_msg = response.json().get(\"error\", {}).get(\"message\", response.text)\n",
    "            raise Exception(f\"Backup creation failed (HTTP {response.status_code}): {error_msg}\")\n",
    "        \n",
    "        # Step 3: Wait and verify backup exists in BACKUP folder\n",
    "        print(f\"  → Waiting for backup to complete...\")\n",
    "        time.sleep(3)  # Quick wait for small files\n",
    "        \n",
    "        # Verify backup exists in the backup folder\n",
    "        verify_url = f\"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/items/{backup_parent_id}/children\"\n",
    "        \n",
    "        for attempt in range(10):  # Max 10 attempts = 30 seconds\n",
    "            verify_response = requests.get(verify_url, headers=headers)\n",
    "            \n",
    "            if verify_response.status_code == 200:\n",
    "                items = verify_response.json().get(\"value\", [])\n",
    "                backup_exists = any(item.get(\"name\") == backup_name for item in items)\n",
    "                \n",
    "                if backup_exists:\n",
    "                    print(f\"  ✓ Backup confirmed: {backup_name}\")\n",
    "                    return {\n",
    "                        \"success\": True,\n",
    "                        \"backup_name\": backup_name\n",
    "                    }\n",
    "            \n",
    "            if attempt < 9:\n",
    "                time.sleep(3)\n",
    "        \n",
    "        # If we reach here, backup not verified - STOP PROCESS\n",
    "        raise Exception(f\"Backup '{backup_name}' not found after 30 seconds - process stopped\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Re-raise exception to stop the entire process\n",
    "        error_msg = f\"BACKUP FAILED: {str(e)}\"\n",
    "        print(f\"  ✗ {error_msg}\")\n",
    "        raise Exception(error_msg)\n",
    "\n",
    "def get_parent_folder_id(folder_path, headers, SITE_ID, DRIVE_ID):\n",
    "    \"\"\"\n",
    "    Get the folder ID from folder path for backup location\n",
    "    \"\"\"\n",
    "    try:\n",
    "        folder_url = f\"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{folder_path}\"\n",
    "        response = requests.get(folder_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json().get(\"id\")\n",
    "        else:\n",
    "            print(f\"  ⚠ Could not get parent folder ID: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Error getting parent folder ID: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def list_all_files(folder_url, folder_path, headers, SITE_ID, DRIVE_ID, parent_folder=None):\n",
    "    \"\"\"Recursively collect all files from a folder and its subfolders.\"\"\"\n",
    "    result = []\n",
    "\n",
    "    r = requests.get(folder_url, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        print(\"Failed to read folder:\", folder_url)\n",
    "        return result\n",
    "\n",
    "    items = r.json().get(\"value\", [])\n",
    "\n",
    "    for item in items:\n",
    "\n",
    "        # If it's a file\n",
    "        if \"file\" in item:\n",
    "            result.append({\n",
    "                \"name\": item[\"name\"],\n",
    "                \"download_url\": item[\"@microsoft.graph.downloadUrl\"],\n",
    "                \"file_id\": item[\"id\"],\n",
    "                \"folder_name\": parent_folder,\n",
    "                \"parent_folder_id\": item.get(\"parentReference\", {}).get(\"id\")\n",
    "            })\n",
    "\n",
    "        # If it's a subfolder → recurse using PATH\n",
    "        if \"folder\" in item:\n",
    "            sub_path = folder_path + \"/\" + item[\"name\"]\n",
    "            sub_url = f\"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{sub_path}:/children\"\n",
    "\n",
    "            # extend recursively\n",
    "            result.extend(\n",
    "                list_all_files(sub_url, sub_path, headers, SITE_ID, DRIVE_ID, parent_folder=item[\"name\"])\n",
    "            )\n",
    "\n",
    "    return result\n",
    "\n",
    "def repair_excel_styles(file_bytes):\n",
    "    \"\"\"\n",
    "    Repair corrupt Excel file dengan menghapus/fix stylesheet yang bermasalah\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Excel adalah ZIP file, extract dan repair\n",
    "        with zipfile.ZipFile(file_bytes, 'r') as zip_ref:\n",
    "            # Baca semua files\n",
    "            file_list = zip_ref.namelist()\n",
    "            \n",
    "            # Create new ZIP in memory\n",
    "            repaired_bytes = BytesIO()\n",
    "            with zipfile.ZipFile(repaired_bytes, 'w', zipfile.ZIP_DEFLATED) as new_zip:\n",
    "                \n",
    "                for filename in file_list:\n",
    "                    file_data = zip_ref.read(filename)\n",
    "                    \n",
    "                    # Skip atau fix styles.xml yang bermasalah\n",
    "                    if filename == 'xl/styles.xml':\n",
    "                        try:\n",
    "                            # Parse dan clean styles\n",
    "                            root = ET.fromstring(file_data)\n",
    "                            \n",
    "                            # Hapus cellStyleXfs yang bermasalah\n",
    "                            ns = {'': 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'}\n",
    "                            for elem in root.findall('.//cellStyleXfs', ns):\n",
    "                                # Kosongkan atau set minimal\n",
    "                                elem.clear()\n",
    "                                elem.set('count', '0')\n",
    "                            \n",
    "                            # Rebuild XML\n",
    "                            file_data = ET.tostring(root, encoding='utf-8')\n",
    "                            print(\"    → Repaired styles.xml\")\n",
    "                        except:\n",
    "                            print(\"    → Using original styles.xml\")\n",
    "                    \n",
    "                    # Write file ke new ZIP\n",
    "                    new_zip.writestr(filename, file_data)\n",
    "            \n",
    "            repaired_bytes.seek(0)\n",
    "            return repaired_bytes\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"    → Repair failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def read_excel_with_repair(file_bytes, sheet_name=0, header=0, filename=\"file.xlsx\"):\n",
    "    \"\"\"\n",
    "    Read Excel file using auto-repair if that file corrupt\n",
    "    \"\"\"\n",
    "    methods = [\n",
    "        {\n",
    "            'name': 'Standard openpyxl',\n",
    "            'func': lambda fb: pd.read_excel(fb, sheet_name=sheet_name, header=header, engine='openpyxl')\n",
    "        },\n",
    "        {\n",
    "            'name': 'Calamine engine (fast, ignore styles)',\n",
    "            'func': lambda fb: pd.read_excel(fb, sheet_name=sheet_name, header=header, engine='calamine')\n",
    "        },\n",
    "        {\n",
    "            'name': 'xlrd (for older formats)',\n",
    "            'func': lambda fb: pd.read_excel(fb, sheet_name=sheet_name, header=header, engine='xlrd')\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Try standard methods first\n",
    "    for method in methods:\n",
    "        try:\n",
    "            file_bytes.seek(0)\n",
    "            df = method['func'](file_bytes)\n",
    "            print(f\"    ✓ Success with: {method['name']}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ {method['name']}: {type(e).__name__}\")\n",
    "            continue\n",
    "    \n",
    "    # If all failed, try repair\n",
    "    print(\"    → Attempting to repair file...\")\n",
    "    file_bytes.seek(0)\n",
    "    repaired = repair_excel_styles(file_bytes)\n",
    "    \n",
    "    if repaired:\n",
    "        try:\n",
    "            df = pd.read_excel(repaired, sheet_name=sheet_name, header=header, engine='openpyxl')\n",
    "            print(f\"    ✓ Success after repair!\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Still failed after repair: {e}\")\n",
    "    \n",
    "    # Last resort: manual extraction dengan openpyxl\n",
    "    print(\"    → Trying manual data extraction...\")\n",
    "    try:\n",
    "        from openpyxl import load_workbook\n",
    "        from openpyxl.styles import Font, PatternFill  # Import untuk default styles\n",
    "        \n",
    "        file_bytes.seek(0)\n",
    "        \n",
    "        # Save to temp file karena load_workbook butuh file path untuk repair mode\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp:\n",
    "            tmp.write(file_bytes.read())\n",
    "            tmp_path = tmp.name\n",
    "        \n",
    "        try:\n",
    "            # Repair on-the-fly dengan keep_links=False\n",
    "            from openpyxl.reader.excel import load_workbook as openpyxl_load\n",
    "            \n",
    "            # Patch openpyxl untuk skip broken styles\n",
    "            import openpyxl.styles.stylesheet\n",
    "            original_init = openpyxl.styles.stylesheet.Stylesheet.__init__\n",
    "            \n",
    "            def patched_init(self, *args, **kwargs):\n",
    "                try:\n",
    "                    original_init(self, *args, **kwargs)\n",
    "                except IndexError:\n",
    "                    # If styles broken, use minimal defaults\n",
    "                    print(\"    → Using default styles (stylesheet broken)\")\n",
    "                    self.named_styles = []\n",
    "            \n",
    "            openpyxl.styles.stylesheet.Stylesheet.__init__ = patched_init\n",
    "            \n",
    "            # Load workbook\n",
    "            wb = load_workbook(tmp_path, data_only=True, keep_links=False)\n",
    "            ws = wb.active if isinstance(sheet_name, int) and sheet_name == 0 else wb[sheet_name]\n",
    "            \n",
    "            # Extract data\n",
    "            data = list(ws.values)\n",
    "            \n",
    "            # Restore original init\n",
    "            openpyxl.styles.stylesheet.Stylesheet.__init__ = original_init\n",
    "            \n",
    "            # Create DataFrame\n",
    "            if header is not None and len(data) > header:\n",
    "                df = pd.DataFrame(data[header+1:], columns=data[header])\n",
    "            else:\n",
    "                df = pd.DataFrame(data)\n",
    "            \n",
    "            print(f\"    ✓ Manual extraction successful!\")\n",
    "            return df\n",
    "            \n",
    "        finally:\n",
    "            # Cleanup temp file\n",
    "            try:\n",
    "                os.unlink(tmp_path)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Manual extraction failed: {type(e).__name__} - {str(e)[:100]}\")\n",
    "    \n",
    "    raise Exception(f\"Cannot read {filename} with any method\")\n",
    "\n",
    "\n",
    "def read_data(FOLDER_PATH, FILE_PATTERN, SHEET_NAME, HEADER, TOKEN, SITE_ID, DRIVE_ID, CSVDelimiter, NeedBackup, backup_folder_path):\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {TOKEN}\"}\n",
    "\n",
    "    folder_url = f\"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{FOLDER_PATH}:/children\"\n",
    "\n",
    "    # Ambil semua file + recursive ke subfolder\n",
    "    all_files = list_all_files(folder_url, FOLDER_PATH, headers, SITE_ID, DRIVE_ID)\n",
    "    \n",
    "    # Filter by regex\n",
    "    matched_files = [f for f in all_files if re.fullmatch(FILE_PATTERN, f[\"name\"])]\n",
    "\n",
    "    if not matched_files:\n",
    "        print(\"No files matched:\", FILE_PATTERN)\n",
    "        return None, None\n",
    "\n",
    "    root_only = all(f[\"folder_name\"] is None for f in matched_files)\n",
    "    if root_only and len(matched_files) > 1:\n",
    "        print(f\"Multiple root-level files matched pattern. Taking one: {matched_files[0]['name']}\")\n",
    "        matched_files = [matched_files[0]]\n",
    "\n",
    "    df_list = []\n",
    "    TableName = None\n",
    "\n",
    "    for f in matched_files:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Reading: {f['name']}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # CREATE BACKUP BEFORE READING\n",
    "        if NeedBackup == 'Y':\n",
    "            backup_result = backup_file_in_sharepoint(\n",
    "                file_id=f[\"file_id\"],\n",
    "                file_name=f[\"name\"],\n",
    "                parent_folder_id=f.get(\"parent_folder_id\"),\n",
    "                headers=headers,\n",
    "                SITE_ID=SITE_ID,\n",
    "                DRIVE_ID=DRIVE_ID,\n",
    "                backup_folder_path = backup_folder_path\n",
    "            )\n",
    "            \n",
    "            if backup_result:\n",
    "                print(f\"  ✓ Backup completed: {backup_result['backup_name']}\")\n",
    "            else:\n",
    "                print(f\"  ⚠ Backup skipped or failed - continuing with read\")\n",
    "        \n",
    "        TableName = f['name']\n",
    "        r_file = requests.get(f[\"download_url\"], headers=headers)\n",
    "\n",
    "        if r_file.status_code != 200:\n",
    "            print(f\"  ✗ Failed to download (status {r_file.status_code})\")\n",
    "            continue\n",
    "\n",
    "        file_bytes = BytesIO(r_file.content)\n",
    "        file_size = len(r_file.content)\n",
    "        print(f\"  File size: {file_size:,} bytes\")\n",
    "        \n",
    "        if file_size == 0:\n",
    "            print(f\"  ✗ File is empty (0 bytes)\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Excel\n",
    "            if f[\"name\"].lower().endswith(\".xlsx\"):\n",
    "                df = read_excel_with_repair(\n",
    "                    file_bytes, \n",
    "                    sheet_name=SHEET_NAME if SHEET_NAME else 0,\n",
    "                    header=HEADER,\n",
    "                    filename=f[\"name\"]\n",
    "                )\n",
    "                \n",
    "                if df.empty:\n",
    "                    print(f\"  ⚠ Warning: DataFrame is empty after reading\")\n",
    "                    # continue\n",
    "                \n",
    "                print(f\"  → Result: {len(df)} rows × {len(df.columns)} columns\")\n",
    "            \n",
    "            # Excel xls\n",
    "            elif f[\"name\"].lower().endswith(\".xls\"):\n",
    "                df = read_excel_with_repair(\n",
    "                    file_bytes, \n",
    "                    sheet_name=SHEET_NAME if SHEET_NAME else 0,\n",
    "                    header=HEADER,\n",
    "                    filename=f[\"name\"]\n",
    "                )\n",
    "                \n",
    "                if df.empty:\n",
    "                    print(f\"  ⚠ Warning: DataFrame is empty after reading\")\n",
    "                    # continue\n",
    "                \n",
    "                print(f\"  → Result: {len(df)} rows × {len(df.columns)} columns\")\n",
    "                \n",
    "            # CSV auto delimiter\n",
    "            elif f[\"name\"].lower().endswith(\".csv\"):\n",
    "                text = file_bytes.getvalue().decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "                valid_delimiters = [\"comma\", \"semicolon\", \"tab\", \"pipe\"]\n",
    "                if CSVDelimiter not in valid_delimiters:\n",
    "                    raise ValueError(f\"Invalid CSVDelimiter: {CSVDelimiter}\")\n",
    "\n",
    "                # Mapping CSVDelimiter to actual delimiter\n",
    "                if CSVDelimiter == \"comma\":\n",
    "                    delimiter = \",\"\n",
    "                elif CSVDelimiter == \"semicolon\":\n",
    "                    delimiter = \";\"\n",
    "                elif CSVDelimiter == \"tab\":\n",
    "                    delimiter = \"\\t\"\n",
    "                elif CSVDelimiter == \"pipe\":\n",
    "                    delimiter = \"|\"\n",
    "\n",
    "                df = pd.read_csv(\n",
    "                    io.StringIO(text),\n",
    "                    delimiter=delimiter,\n",
    "                    header=HEADER\n",
    "                )\n",
    "\n",
    "                print(f\"  → Result: {len(df)} rows × {len(df.columns)} columns\")\n",
    "\n",
    "            if len(matched_files) > 1:\n",
    "                folder_name = f.get('folder_name')\n",
    "                df['Source'] = folder_name if folder_name else 'Root'\n",
    "\n",
    "            df_list.append(df)\n",
    "            print(f\"  ✓ Successfully added to dataset\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ SKIPPED: {type(e).__name__}\")\n",
    "            print(f\"     {str(e)[:200]}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    if not df_list:\n",
    "        print(\"❌ No files processed successfully\")\n",
    "        return None, None\n",
    "    \n",
    "\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"✓ SUCCESS! Combined dataset:\")\n",
    "    print(f\"  - Total rows: {final_df.shape[0]:,}\")\n",
    "    print(f\"  - Total columns: {final_df.shape[1]}\")\n",
    "    print(f\"  - Files processed: {len(df_list)}\")\n",
    "    print('='*60)\n",
    "\n",
    "    return final_df, TableName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b676d",
   "metadata": {},
   "source": [
    "### Call the func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "817c3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is generated from runtime parameters. Learn more: https://go.microsoft.com/fwlink/?linkid=2161015\n",
    "\n",
    "FilePattern = \"Online Retail.xlsx\"\n",
    "FolderPath = \"Fabric_Excel_Files/Test_Excel/Self_service_framework\"\n",
    "SheetName = \"Online Retail\"\n",
    "Header = \"1\"\n",
    "NeedBackup = \"N\"\n",
    "FlexibleSchema = \"N\"\n",
    "CSVDelimiter = None\n",
    "BackupFolderPath = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0876a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Header = int(Header)\n",
    "Header -= 1\n",
    "FolderURL = f\"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{FolderPath}:/children\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88d72761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Reading: Online Retail.xlsx\n",
      "============================================================\n",
      "  File size: 24,430,215 bytes\n",
      "    ✓ Success with: Standard openpyxl\n",
      "  → Result: 541909 rows × 8 columns\n",
      "  ✓ Successfully added to dataset\n",
      "\n",
      "============================================================\n",
      "✓ SUCCESS! Combined dataset:\n",
      "  - Total rows: 541,909\n",
      "  - Total columns: 8\n",
      "  - Files processed: 1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "df, DynamicTableName = read_data(FolderPath, FilePattern, SheetName, Header, Token, SITE_ID, DRIVE_ID, CSVDelimiter, NeedBackup, BackupFolderPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64dba428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>451950</th>\n",
       "      <td>575300</td>\n",
       "      <td>47566</td>\n",
       "      <td>PARTY BUNTING</td>\n",
       "      <td>12</td>\n",
       "      <td>2011-11-09 12:30:00</td>\n",
       "      <td>4.95</td>\n",
       "      <td>13804.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245703</th>\n",
       "      <td>558628</td>\n",
       "      <td>22232</td>\n",
       "      <td>JIGSAW TOADSTOOLS 3 PIECE</td>\n",
       "      <td>16</td>\n",
       "      <td>2011-06-30 17:59:00</td>\n",
       "      <td>0.59</td>\n",
       "      <td>12626.0</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371263</th>\n",
       "      <td>569216</td>\n",
       "      <td>22558</td>\n",
       "      <td>CLOTHES PEGS RETROSPOT PACK 24</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-10-02 12:41:00</td>\n",
       "      <td>1.65</td>\n",
       "      <td>15555.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397925</th>\n",
       "      <td>571214</td>\n",
       "      <td>23199</td>\n",
       "      <td>JUMBO BAG APPLES</td>\n",
       "      <td>5</td>\n",
       "      <td>2011-10-14 12:56:00</td>\n",
       "      <td>2.08</td>\n",
       "      <td>14390.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126389</th>\n",
       "      <td>547074</td>\n",
       "      <td>22174</td>\n",
       "      <td>PHOTO CUBE</td>\n",
       "      <td>12</td>\n",
       "      <td>2011-03-20 14:35:00</td>\n",
       "      <td>1.65</td>\n",
       "      <td>13102.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       InvoiceNo StockCode                      Description  Quantity  \\\n",
       "451950    575300     47566                    PARTY BUNTING        12   \n",
       "245703    558628     22232        JIGSAW TOADSTOOLS 3 PIECE        16   \n",
       "371263    569216     22558  CLOTHES PEGS RETROSPOT PACK 24          3   \n",
       "397925    571214     23199                 JUMBO BAG APPLES         5   \n",
       "126389    547074     22174                       PHOTO CUBE        12   \n",
       "\n",
       "               InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "451950 2011-11-09 12:30:00       4.95     13804.0  United Kingdom  \n",
       "245703 2011-06-30 17:59:00       0.59     12626.0         Germany  \n",
       "371263 2011-10-02 12:41:00       1.65     15555.0  United Kingdom  \n",
       "397925 2011-10-14 12:56:00       2.08     14390.0  United Kingdom  \n",
       "126389 2011-03-20 14:35:00       1.65     13102.0  United Kingdom  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
